{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "communist-dallas",
   "metadata": {},
   "source": [
    "### Informações e estatísticas da base de redações Essays Text Dataset (ETD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-execution",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from glob import glob\n",
    "\n",
    "PATH_REDACOES = \"./redacoes/TXT/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-interim",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conta da quantidade de arquivos de redações\n",
    "\n",
    "path = Path(PATH_REDACOES)\n",
    "\n",
    "all_arqs = [str(x) for x in list(path.rglob(\"*\"))]\n",
    "arqs_essays = sorted(list(filter(lambda x: \".html\" != str(x)[-5:], all_arqs)), \n",
    "                     key=lambda x: int(x.split('_')[1]))\n",
    "arqs_annotated = list(filter(lambda x: \".html\" == str(x)[-5:], all_arqs))\n",
    "#arqs_essays = list(path.glob(\"*\"))\n",
    "#arqs_annotated = list(path.rglob('*.html'))\n",
    "\n",
    "print(f\"Total de arquivos de redações: {len(arqs_essays)}\")        \n",
    "print(f\"Total de arquivos de redações anotadas: {len(arqs_annotated)}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-stake",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('pt_core_news_lg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-agenda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contagem de palavras das redacoes\n",
    "total_palavras = 0\n",
    "for arq_essay in arqs_essays:\n",
    "    texto = \"\"\n",
    "    with open(arq_essay, \"r\") as f_essay:\n",
    "        texto = \"\\n\".join(f_essay.readlines())\n",
    "\n",
    "    doc = nlp(texto)\n",
    "    #words = [token.text for token in doc if token.is_stop != True and token.is_punct != True]\n",
    "    words = [token.text for token in doc if token.is_punct != True]\n",
    "    total_palavras += len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-fault",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total de palavras de todas as redações: {total_palavras}\")\n",
    "print(f\"Média de palavras por redação: {round(total_palavras/len(arqs_essays))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-fraud",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ex = nlp(arqs_essays[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-knitting",
   "metadata": {},
   "source": [
    "Algumas redações estavam com pontuações sem espaços subsequentes ou parênteses antecedentes,\n",
    "bem como algumas aspas e aspas duplas estavam desnormalizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-wilson",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_redacao = arqs_essays[0]\n",
    "\n",
    "texto_redacao = \"\"\n",
    "with open(path_redacao) as f:\n",
    "    texto_redacao = \"\".join(f.readlines())\n",
    "\n",
    "texto_redacao "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-encyclopedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Corrige os textos dos arquivos, alterando\n",
    "# principalmente, situações de pontuação \n",
    "# sem espaço subsequente\n",
    "def ajusta_texto(texto):\n",
    "    \n",
    "    pats = [r\"\\..[A-Za-z^\\s]\",   # ponto \".\" sem espaço subsequente\n",
    "            r\",.[A-Za-z^\\s]\",    # virgula \",\" sem espaço subsequente\n",
    "            r\"\\(.[A-Za-z^\\s]\",    # parentese \"(\" sem espaço antecedente\n",
    "            r\".[A-Za-z^\\s]\\)\"    # parentese \"(\" sem espaço subsequente\n",
    "           ]\n",
    "    chars = [\".\", \",\", \"(\", \")\"]\n",
    "     \n",
    "    for c, pat in zip(chars, pats):\n",
    "        matches = re.findall(pat, texto, re.MULTILINE)         \n",
    "\n",
    "        for m in matches:\n",
    "            pos_m = texto.find(m)\n",
    "            pos_dot = m.find(c)\n",
    "            \n",
    "            if c == \"(\":\n",
    "                if not(texto[pos_m-1] == \" \"):\n",
    "                    m_rep =  f\" {m}\"\n",
    "                    texto = texto.replace(m, m_rep)\n",
    "            else:\n",
    "                pos_m = pos_m + pos_dot\n",
    "                if ((pos_m+1) < len(texto)):\n",
    "                    if not(texto[pos_m+1] == \" \"):\n",
    "                        if c == ',' and texto[pos_m+1] in \"0123456789\":\n",
    "                            continue\n",
    "                        m_rep = f\"{m[:pos_dot+1]} {m[pos_dot+1:]}\"\n",
    "                        texto = texto.replace(m, m_rep)\n",
    "    \n",
    "    texto = texto.replace(\"\\'\\'\", \"\\\"\")\n",
    "    texto = texto.replace(\" .\", \".\")\n",
    "    texto = texto.replace(\" ,\", \",\")\n",
    "    \n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-directory",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(texto_redacao)\n",
    "print(100*\"=\")\n",
    "print(ajusta_texto(texto_redacao))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-bearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"1,23 milhões ver sem amigos , pois faz. parte(ou mais parte),ou não faria e,outra vez(quem sabe) . \"\n",
    "ajusta_texto(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-edward",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando um arquivo contendo todas as redações\n",
    "PATH_ALL = \"./redacoes/\"\n",
    "l_separador = \"\\n\"\n",
    "\n",
    "for arq_essay in arqs_essays:\n",
    "    texto = \"\"\n",
    "    texto += f\"<ARQUIVO: \\\"{arq_essay}\\\">\"\n",
    "    texto += l_separador    \n",
    "    with open(arq_essay, \"r\") as f_essay:\n",
    "        texto += ajusta_texto(\"\".join(f_essay.readlines()))\n",
    "    \n",
    "    texto += l_separador        \n",
    "    with open(f\"{PATH_ALL}all_texts.txt\", \"a\") as f_todos:\n",
    "        f_todos.write(texto)\n",
    "       \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-store",
   "metadata": {},
   "source": [
    "### Avaliando os erros ortograficos presentes nos arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-consultation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import language_tool_python as ltp\n",
    "\n",
    "tool = ltp.LanguageTool('pt-BR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-digest",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_redacao = arqs_essays[0]\n",
    "\n",
    "texto_redacao = \"\"\n",
    "with open(path_redacao) as f:\n",
    "    texto_redacao = \"\".join(f.readlines())\n",
    "\n",
    "texto_redacao \n",
    "erros = tool.check(ajusta_texto(texto_redacao))\n",
    "\n",
    "erros"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
