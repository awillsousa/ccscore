{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "vanilla-designation",
   "metadata": {},
   "source": [
    "### Processo de Extração de Entidades e Recuperação de Dados da Wikidata\n",
    "Neste notebook serão executadas as ações de recuperação das entidades do corpus de redações e \n",
    "também, a recuperação de dados dessas entidades, a partir da wikidata, para assim gerar\n",
    "uma lista de entidades e os seus possíveis alias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-frequency",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import wikidata\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-option",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_redacoes = pd.read_json(\"../ccscore/data/todas_redacoes_norm.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-newfoundland",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### TESTE #######\n",
    "\n",
    "\n",
    "df_redacoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-backup",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####### TESTE #######\n",
    "\n",
    "num_red = 100\n",
    "print(df_redacoes.loc[num_red, [\"original\"]][0])\n",
    "print()\n",
    "print(df_redacoes.loc[num_red, [\"criteria\"]][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-bowling",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####### TESTE #######\n",
    "\n",
    "num_red = 0\n",
    "original = df_redacoes.loc[num_red, [\"original\"]][0]\n",
    "fixed = df_redacoes.loc[num_red, [\"fixed\"]][0]\n",
    "print(original)\n",
    "print()\n",
    "#print(fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-worst",
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper_dbpedia as dbpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-creativity",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### TESTE #######\n",
    "\n",
    "entities = dbpedia.get_dbpedia_entries(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-shelter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####### TESTE #######\n",
    "\n",
    "dbpedia.get_dbpedia_entries('aço')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-disney",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### TESTE #######\n",
    "\n",
    "import wikipedia\n",
    "\n",
    "wikipedia.set_lang(\"pt\")\n",
    "wikipedia.search(\"aço\")\n",
    "page = wikipedia.page(\"aço\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-korean",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "####### TESTE #######\n",
    "\n",
    "dir(page)\n",
    "\n",
    "atr_vals = [\n",
    "page.categories,\n",
    "page.content,\n",
    "#page.coordinates,\n",
    "page.html,\n",
    "page.images,\n",
    "page.links,\n",
    "page.original_title,\n",
    "page.pageid,\n",
    "page.parent_id,\n",
    "page.references,\n",
    "page.revision_id,\n",
    "page.section,\n",
    "page.sections,\n",
    "page.summary,\n",
    "page.title,\n",
    "page.url\n",
    "]\n",
    "\n",
    "print(\"\\n\".join([str(x) for x in atr_vals if not (x is None)]))\n",
    "#\"Elemento\" in page.links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-communication",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####### TESTE #######\n",
    "\n",
    "import requests\n",
    "\n",
    "item_wiki=\"pr\"   \n",
    "#URL_REQUEST=f\"https://pt.wikipedia.org/w/api.php?action=query&generator=links&format=json&redirects=1&prop=pageprops&gpllimit=50&ppprop=wikibase_item&titles={item_wiki}\"\n",
    "URL_REQUEST=f\"https://pt.wikipedia.org/w/api.php?action=query&generator=links&format=json&redirects=1&prop=pageprops&gpllimit=50&ppprop=wikibase_item&titles={item_wiki}\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(URL_REQUEST)    \n",
    "    response.raise_for_status()\n",
    "except HTTPError as http_err:\n",
    "    print(f'Erro HTTP: {http_err}') \n",
    "except Exception as err:\n",
    "    print(f'Erro genérico: {err}')\n",
    "else:\n",
    "    print('Successo!')\n",
    "\n",
    "    \n",
    "#print(response.content)\n",
    "dados_wiki_item = json.loads(response.content)\n",
    "s_json = response.content\n",
    "\n",
    "print(json.dumps(dados_wiki_item, indent=3))\n",
    "\n",
    "\n",
    "\n",
    "#print(dados_wiki_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-shield",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.exceptions import HTTPError\n",
    "import json\n",
    "import pydash\n",
    "\n",
    "def get_wikipedia_page(item_wiki, show_raw=False):    \n",
    "    URL_REQUEST=f\"https://pt.wikipedia.org/w/api.php?action=query&prop=pageprops&format=json&titles={item_wiki}\"\n",
    "    try:\n",
    "        response = requests.get(URL_REQUEST)    \n",
    "        response.raise_for_status()\n",
    "    except HTTPError as http_err:\n",
    "        print(f'Erro HTTP: {http_err}') \n",
    "    except Exception as err:\n",
    "        print(f'Erro genérico: {err}')\n",
    "        \n",
    "    dados_wiki_item = json.loads(response.content)\n",
    "\n",
    "    if show_raw:\n",
    "        print(json.dumps(dados_wiki_item, indent=3))\n",
    "\n",
    "    wikibase_item = None        \n",
    "    type_item='REDIRECT_PAGE'\n",
    "    title_item = \"\"\n",
    "    is_disambiguation_page = False\n",
    "    is_redirect_page = True\n",
    "    query = dados_wiki_item['query'] if 'query' in dados_wiki_item.keys() else None\n",
    "    pages = query['pages'] if 'pages' in query.keys() else None\n",
    "    for p in pages.values():\n",
    "        for k,v in p.items():\n",
    "            if k == 'pageprops':\n",
    "                is_redirect_page = False\n",
    "                if 'wikibase_item' in v.keys():\n",
    "                    type_item='WIKIBASE_ITEM'\n",
    "                    wikibase_item = v['wikibase_item']\n",
    "                if 'disambiguation' in v.keys():\n",
    "                    type_item='DISAMBIGUATION_PAGE'\n",
    "                    is_disambiguation_page = True\n",
    "            elif k == 'title':\n",
    "                title_item = v\n",
    "      \n",
    "    return type_item, wikibase_item, title_item\n",
    "\n",
    "\n",
    "def get_redirect_page(item_wiki, show_raw=False):\n",
    "    URL_REQUEST=f\"https://pt.wikipedia.org/w/api.php?action=query&&redirects&format=json&titles={item_wiki}\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(URL_REQUEST)    \n",
    "        response.raise_for_status()\n",
    "    except HTTPError as http_err:\n",
    "        print(f'Erro HTTP: {http_err}') \n",
    "    except Exception as err:\n",
    "        print(f'Erro genérico: {err}')    \n",
    "\n",
    "    dados_wiki_item = json.loads(response.content)\n",
    "\n",
    "    if show_raw:\n",
    "        print(json.dumps(dados_wiki_item, indent=3))\n",
    "\n",
    "    #redirect_to = pydash.get(dados_wiki_item, 'query.redirects[0].to')\n",
    "    redirects = pydash.get(dados_wiki_item, 'query.redirects')\n",
    "\n",
    "    results = []\n",
    "    if len(redirects) > 0:\n",
    "        for r_item in redirects:\n",
    "            redirect_to = r_item['to']\n",
    "            results.append(\n",
    "                            get_wikipedia_page(item_wiki=redirect_to,\n",
    "                                               show_raw=show_raw)\n",
    "                          )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_disambiguation_page(item_wiki, show_raw=False):\n",
    "    URL_REQUEST=f\"https://pt.wikipedia.org/w/api.php?action=query&generator=links&format=json&redirects=1&prop=pageprops&gpllimit=50&ppprop=wikibase_item&titles={item_wiki}\"\n",
    "    DESCARTE_QIDS = ['Q4167410',   # Wikipedia:Desambiaguação\n",
    "                 'Q151',       # Wikcionário\n",
    "                 'Q4167836',    # Wikimedia category                 \n",
    "                ]  \n",
    "    try:\n",
    "        response = requests.get(URL_REQUEST)    \n",
    "        response.raise_for_status()\n",
    "    except HTTPError as http_err:\n",
    "        print(f'Erro HTTP: {http_err}') \n",
    "    except Exception as err:\n",
    "        print(f'Erro genérico: {err}')    \n",
    "        \n",
    "    dados_wiki_item = json.loads(response.content)\n",
    "\n",
    "    if show_raw:\n",
    "        print(json.dumps(dados_wiki_item, indent=3))\n",
    "        \n",
    "    redirects = []\n",
    "    \n",
    "    redirects_from = pydash.get(dados_wiki_item, 'query.redirects.from')\n",
    "    if not(redirects_from is None):\n",
    "        redirects += redirects_from\n",
    "    \n",
    "    redirects_to = pydash.get(dados_wiki_item, 'query.redirects.to')\n",
    "    if not(redirects_to is None):\n",
    "        redirects += redirects_to\n",
    "    \n",
    "    redirects = set(redirects)\n",
    "\n",
    "    pages = pydash.get(dados_wiki_item, 'query.pages')\n",
    "    \n",
    "    if show_raw:\n",
    "        print(f\"\\n{50*'='}\\n\")\n",
    "        print(json.dumps(pages, indent=3))\n",
    "        print(f\"\\n{50*'='}\\n\")\n",
    "        \n",
    "    results = []\n",
    "    if not(pages is None):\n",
    "        for p_item in pages.values():\n",
    "            p_item_title = p_item['title']\n",
    "            p_wikibase_item = pydash.get(p_item, \"pageprops.wikibase_item\")\n",
    "\n",
    "            # não quero redirects \n",
    "            if p_item_title in redirects:    \n",
    "                continue\n",
    "                \n",
    "            # não quero itens vazios e itens da lista\n",
    "            # de descarte\n",
    "            if p_wikibase_item is None or \\\n",
    "               p_wikibase_item in DESCARTE_QIDS:\n",
    "                continue\n",
    "            \n",
    "            # Recupera a pagina referente ao item\n",
    "            tipo, item, title = get_wikipedia_page(p_item_title, show_raw)\n",
    "            \n",
    "            # Queremos apenas itens da wikidata, sem páginas\n",
    "            # de redirecionamento e desambiguação\n",
    "            if tipo == 'WIKIBASE_ITEM':\n",
    "                results.append((tipo, item, title))\n",
    "                \n",
    "\n",
    "    return results\n",
    "        \n",
    "def get_wikipedia_data(item_wiki, show_raw=False):\n",
    "    tipo, item, title = get_wikipedia_page(item_wiki, show_raw)\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    if tipo == 'WIKIBASE_ITEM':\n",
    "        print(\"O item consultado é um 'item direto da wikipedia'.\")        \n",
    "        result = [ (tipo, item, title) ]\n",
    "    elif tipo == 'REDIRECT_PAGE':    \n",
    "        print(\"O item consultado é uma página de redirecionamento.\")\n",
    "        result_redir = get_redirect_page(item_wiki, show_raw)\n",
    "        \n",
    "        # As vezes a pagina de redirecionamento pode levar\n",
    "        # para uma pagina de desambiguacao\n",
    "        if len(result_redir) == 1 and \\\n",
    "           result_redir[0][0] == 'DISAMBIGUATION_PAGE':\n",
    "            result = get_disambiguation_page(item_wiki, show_raw)\n",
    "        else:\n",
    "            result = result_redir                \n",
    "        \n",
    "    elif tipo == 'DISAMBIGUATION_PAGE':    \n",
    "        print(\"O item consultado é uma página de desambiguação.\")        \n",
    "        result = get_disambiguation_page(item_wiki, show_raw)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-judgment",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### TESTE #######\n",
    "\n",
    "#print(get_wikipedia_page(\"Paraná\", show_raw=True))\n",
    "print(get_wikipedia_page(\"Paint Rock\", show_raw=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-settle",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "####### TESTE #######\n",
    "\n",
    "#results_wikidata = get_wikipedia_data(\"patrão\", show_raw=True)\n",
    "results_wikidata = get_wikipedia_data(\"legislativo\", show_raw=False)\n",
    "\n",
    "#if tipo == 'REDIRECT_PAGE':    \n",
    "#    tipo, item, title = get_redirect_page(\"legislativo\", show_raw=True)\n",
    "    \n",
    "for tipo,item,title in results_wikidata:    \n",
    "    print(tipo)\n",
    "    print()\n",
    "    print(item)\n",
    "    print()\n",
    "    print(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-bridal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.error import HTTPError\n",
    "from wikidata.client import Client\n",
    "\n",
    "STR_PTBR = 'pt-br'\n",
    "STR_PT = 'pt'\n",
    "STR_EN = 'en'\n",
    "\n",
    "def get_str_lang(item):\n",
    "    if isinstance(item, dict):\n",
    "        if STR_PTBR in item.keys():\n",
    "            return STR_PTBR\n",
    "        elif STR_PT in item.keys():\n",
    "            return STR_PT\n",
    "        elif STR_EN in item.keys():\n",
    "            return STR_EN\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def get_field_values(wikidata_item, item_field):\n",
    "    str_lang = get_str_lang(wikidata_item.data[item_field])\n",
    "    values_field = []\n",
    "    if str_lang:\n",
    "        if isinstance(wikidata_item.data[item_field][str_lang], dict):\n",
    "            values_field = [\n",
    "                             elem for k, elem in wikidata_item.data[item_field][str_lang].items() \n",
    "                                  if k == 'value'\n",
    "                           ]\n",
    "        elif isinstance(wikidata_item.data[item_field][str_lang], list):\n",
    "            values_field = [\n",
    "                             elem for x in wikidata_item.data[item_field][str_lang] \n",
    "                                      for k,elem in x.items() if k == 'value'\n",
    "                           ]    \n",
    "        \n",
    "    return values_field\n",
    "\n",
    "def get_wikidata_aliases(wikibase_item):\n",
    "    \n",
    "    aliases_pt_all = []\n",
    "    \n",
    "    aliases_ptbr = pydash.get(wikibase_item.data, 'aliases.pt-br')        \n",
    "    if not(aliases_ptbr is None):\n",
    "        aliases_pt_all += list({ x['value'] for x in aliases_ptbr })\n",
    "       \n",
    "    aliases_pt = pydash.get(wikibase_item.data, 'aliases.pt')        \n",
    "    if not(aliases_pt is None):\n",
    "        aliases_pt_all += list({ \n",
    "                                x['value'] for x in aliases_pt \n",
    "                                    if not(x['value'] in aliases_pt_all)\n",
    "                               })\n",
    "\n",
    "    return list(aliases_pt_all)\n",
    "        \n",
    "    \n",
    "def get_wikidata_descriptions(wikibase_item):\n",
    "    \n",
    "    desc_pt_all = set([])\n",
    "    \n",
    "    desc_ptbr = pydash.get(wikibase_item.data, 'descriptions.pt-br.value')\n",
    "    if not(desc_ptbr is None):\n",
    "        desc_pt_all.add(desc_ptbr)\n",
    "    \n",
    "    desc_pt = pydash.get(wikibase_item.data, 'descriptions.pt.value')\n",
    "    if not(desc_pt is None):\n",
    "        desc_pt_all.add(desc_pt)\n",
    "    \n",
    "    return list(desc_pt_all)\n",
    "    \n",
    "def get_wikidata_labels(wikibase_item):\n",
    "    labels_pt_all = set([])\n",
    "    \n",
    "    labels_ptbr = pydash.get(wikibase_item.data, 'labels.pt-br.value')        \n",
    "    if not(labels_ptbr is None):\n",
    "        labels_pt_all.add(labels_ptbr) \n",
    "    \n",
    "    labels_pt = pydash.get(wikibase_item.data, 'labels.pt.value')    \n",
    "    if not(labels_pt is None):\n",
    "        labels_pt_all.add(labels_pt)\n",
    "    \n",
    "    return list(labels_pt_all)\n",
    "    \n",
    "\n",
    "def get_wikidata_info(wikibase_item):\n",
    "    \n",
    "    client_wikidata = Client()\n",
    "    try:\n",
    "        wikidata_item = client_wikidata.get(wikibase_item, load=True)\n",
    "        \n",
    "        labels_item = get_wikidata_labels(wikidata_item)\n",
    "        descriptions_item = get_wikidata_descriptions(wikidata_item)    \n",
    "        aliases_item = get_wikidata_aliases(wikidata_item)    \n",
    "        \n",
    "        return labels_item, descriptions_item, aliases_item\n",
    "    \n",
    "    except HTTPError as e:\n",
    "        if e.code == 404:\n",
    "            print(f\"Item não encontrado na WikiData - erro {e.code}\")        \n",
    "        else: \n",
    "            print(f\"Erro HTTP\",\"\\n\", f\"Erro {e.code}\") \n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-enforcement",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####### TESTE #######\n",
    "\n",
    "\n",
    "c_wk = Client()\n",
    "\n",
    "item_wkdt = c_wk.get(\"Q15499\")\n",
    "\n",
    "print(item_wkdt.attributes['labels']['pt'])\n",
    "print(item_wkdt.attributes['descriptions']['pt'])\n",
    "print(item_wkdt.attributes['aliases']['pt'])\n",
    "\n",
    "print(f\"descriptions: {get_wikidata_descriptions(item_wkdt)}\")\n",
    "print(f\"labels: {get_wikidata_labels(item_wkdt)}\")\n",
    "print(f\"aliases: {get_wikidata_aliases(item_wkdt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-audit",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_wikientity_data(item_query, show_raw=False):\n",
    "    wikipedia_data = get_wikipedia_data(item_query, show_raw)\n",
    "    items = []\n",
    "    for tipo, item_id, title in wikipedia_data:\n",
    "        labels_item, descriptions_item, aliases_item = get_wikidata_info(item_id)        \n",
    "        item_data = {'id': item_id,\n",
    "                     'title': title,\n",
    "                     'labels': labels_item,\n",
    "                     'descriptions': \" \".join(descriptions_item),\n",
    "                     'aliases': aliases_item}\n",
    "        items.append(item_data)\n",
    "        \n",
    "        if show_raw:            \n",
    "            print(f\"\\nItem consultado: {title} - {item_id}\\n\")            \n",
    "            print(item_data,\"\\n\")\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-boost",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####### TESTE #######\n",
    "item_consulta = \"aço\"\n",
    "get_wikientity_data(item_consulta, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_line():\n",
    "     with open(PATH_TODAS_REDACOES, 'r') as arq_redacoes:\n",
    "            for linha in arq_redacoes:\n",
    "                yield linha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-thunder",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def e_linha_titulo(linha):\n",
    "    return \"<ARQUIVO: \" in linha or \\\n",
    "           linha == \"\"\n",
    "\n",
    "PATH_TODAS_REDACOES=\"../ccscore/data/all_texts.txt\"\n",
    "def get_redacao():\n",
    "    with open(PATH_TODAS_REDACOES, 'r') as arq_redacoes:        \n",
    "        for key,group in itertools.groupby(arq_redacoes, e_linha_titulo):            \n",
    "            if not key:\n",
    "                yield \"\".join(list(group))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gera uma lista com todas as redações\n",
    "\n",
    "i = 0 \n",
    "redacoes = []\n",
    "for r in get_redacao():        \n",
    "    redacoes.append(r)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-finish",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### TESTE #######\n",
    "\n",
    "redacoes[1001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-adolescent",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "MONGO_IP_SERVER = \"willbot\"\n",
    "MONGO_ROOT = \"root\"\n",
    "MONGO_PASS = \"Ktap1mb@!\"\n",
    "def get_mongo_client():\n",
    "    try:\n",
    "        mongoclient = MongoClient(MONGO_IP_SERVER, \n",
    "                              username=MONGO_ROOT, \n",
    "                              password=MONGO_PASS)    \n",
    "        \n",
    "        return mongoclient\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def get_mongo_db(database, mongo_client=None):\n",
    "    if mongo_client is None:\n",
    "        mongo_client = get_mongo_client()\n",
    "    \n",
    "    return mongo_client[database]\n",
    "                \n",
    "\n",
    "def get_mongo_collection(collection_name, mongo_db):    \n",
    "    dic_collection = None\n",
    "    try:\n",
    "        if collection_name not in mongo_db.list_collection_names():\n",
    "            dic_collection = mongo_db.create_collection(collection_name)\n",
    "        else:\n",
    "            dic_collection = mongo_db[collection_name]        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao tentar criar a collection {collection_name}\")\n",
    "        print(str(e))\n",
    "\n",
    "    return dic_collection\n",
    "\n",
    "def insere_entidade_db(collection, entity_obj):  \n",
    "    '''\n",
    "    Insere um objeto entidade no MongoDB\n",
    "    Esperar um objeto do tipo:\n",
    "    r_jsonl = {'id': id_item,\n",
    "                   'name': titulo,\n",
    "                   'description': descricao,\n",
    "                   'label': \"\"    \n",
    "                    }\n",
    "    '''\n",
    "    qtd_ents = collection.count_documents({'name': entity_obj['name']})\n",
    "    \n",
    "    if qtd_ents == 0:    \n",
    "        try:\n",
    "            id_inserted = collection.insert_one(entity_obj).inserted_id\n",
    "\n",
    "            return id_inserted\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao tentar inserir a entidade {entity_obj['name']}\")\n",
    "            print(str(e))\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def get_entity_by_qid(collection, id_entity):    \n",
    "    q_entity = collection.find_one({'id': id_entity})\n",
    "    \n",
    "    return q_entity\n",
    "\n",
    "\n",
    "def reajusta_probs(num_items):\n",
    "    '''\n",
    "    Ajusta os valores de probabilidade, distribuindo\n",
    "    os valores de acordo com a quantidade de itens\n",
    "    '''\n",
    "    l_r = num_items*[round(1/num_items,2)]\n",
    "    \n",
    "    if sum(l_r) < 1.0:\n",
    "        l_r[0] += (1 - sum(l_r))\n",
    "    elif sum(l_r) > 1.0:\n",
    "        l_r[-1] -= (sum(l_r)-1)\n",
    "    \n",
    "    return l_r\n",
    "    \n",
    "def insere_alias_db(collection, alias_obj):\n",
    "    '''\n",
    "    Insere um objeto alias no MongoDB\n",
    "    Esperar um objeto do tipo:\n",
    "    a_jsonl = {'alias': titulo,\n",
    "                   'entities': [id_item],\n",
    "                   'probabilities': [1.0]}\n",
    "    '''\n",
    "    # Verifica se o alias inserido já existe    \n",
    "    qtd_alias = collection.count_documents({'alias': alias_obj['alias']})\n",
    "    \n",
    "    if qtd_alias == 0:  # não existe no db                \n",
    "        try:\n",
    "            id_inserted = collection.insert_one(alias_obj).inserted_id\n",
    "\n",
    "            return id_inserted\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao tentar inserir aliases na base. \\n{str(e)}\")\n",
    "            return None\n",
    "    else:   # substitui             \n",
    "        doc_alias = collection.find_one({'alias': alias_obj['alias']})\n",
    "        qt_entities = len(doc_alias['entities'])\n",
    "        new_entity_list = list(set(doc_alias['entities']+alias_obj['entities']))\n",
    "        new_qt_entity_list = len(new_entity_list)\n",
    "        if qt_entities < new_qt_entity_list: # alguma entitidade foi acrescida?\n",
    "            new_probs = reajusta_probs(new_qt_entity_list)\n",
    "            result = collection.update_one(\n",
    "                            {\"_id\" : doc_alias[\"_id\"]},\n",
    "                            {\n",
    "                                \"$set\": {'entities': new_entity_list,\n",
    "                                    'probabilities': new_probs}\n",
    "                            },\n",
    "                            upsert=True)\n",
    "            if not result.acknowledged:\n",
    "                print(f\"Erro ao tentar atualizar o documento {doc_alias['_id']}\")\n",
    "        else:  # não há nada de novo para acrescentar\n",
    "            return None\n",
    "\n",
    "\n",
    "def get_alias_by_name(collection, name_alias):\n",
    "    mongo_alias = collection.find_one({'alias': name_alias})\n",
    "    \n",
    "    if not (mongo_alias is None):        \n",
    "        return mongo_alias\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-accused",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kb_mongo = get_mongo_db(\"kb\")\n",
    "raw_kb = get_mongo_collection(\"raw_kb\", kb_mongo)\n",
    "entidades_kb = get_mongo_collection(\"entidades_kb\", kb_mongo)\n",
    "alias_kb = get_mongo_collection(\"alias_kb\", kb_mongo)\n",
    "\n",
    "for num_redacao, redacao in enumerate(redacoes[:10]):\n",
    "    if len(redacao) < 5:\n",
    "        continue\n",
    "\n",
    "    print()\n",
    "    print(50*\"=\")\n",
    "    print(f\"Processando redação {num_redacao}\")\n",
    "    print(50*\"=\")\n",
    "    entidades_dbpedia = dbpedia.get_dbpedia_entries(redacao)\n",
    "\n",
    "    for t_ent, d_ent in entidades_dbpedia.items():\n",
    "        try:\n",
    "            print(f\"Processando entidade {t_ent}\")\n",
    "            \n",
    "            #qtd_ents = raw_kb.count_documents({'dbpedia_text': d_ent['dbpedia_text']})\n",
    "            qtd_ents = raw_kb.count_documents({'dbpedia_text': d_ent['raw_text']})\n",
    "            \n",
    "            if qtd_ents > 0: # inserir apenas um documento por item\n",
    "                print(f\"Entidade {t_ent} já existe na base de dados.\")\n",
    "            else:                 \n",
    "                id_inserido = raw_kb.insert_one(d_ent).inserted_id\n",
    "                print(f\"Inserido id {id_inserido}\")\n",
    "            \n",
    "                #wikidata_items = get_wikientity_data(d_ent['dbpedia_text'])\n",
    "                wikidata_items = get_wikientity_data(d_ent['raw_text']) \n",
    "                for item_data in wikidata_items:\n",
    "                    print(f\"\\n\\nitem_data {item_data}\\n\\n\")\n",
    "                    item_data_id   = item_data['id']\n",
    "                    item_data_name = item_data['title']\n",
    "                    item_data_desc = item_data['descriptions']\n",
    "                    item_data_labels = item_data['labels']\n",
    "                    item_data_aliases = item_data['aliases']\n",
    "\n",
    "                    #print(item_data_id)\n",
    "                    #print(item_data_name)\n",
    "                    #print(item_data_desc)\n",
    "                    #print(item_data_labels)\n",
    "                    #print(item_data_aliases)\n",
    "\n",
    "                    # Insere entidade \n",
    "                    r_jsonl = {\n",
    "                               'id': item_data_id,\n",
    "                               'name': item_data_name,\n",
    "                               'description': item_data_desc,\n",
    "                               'label': \"\",  # este campo é um indicador da base de conhecimento \n",
    "                                             # diferente do label retornado da wikidata\n",
    "                              }\n",
    "                    insere_entidade_db(entidades_kb, r_jsonl)\n",
    "\n",
    "                    # Insere um alias equivalente a entidade\n",
    "                    a_jsonl = {\n",
    "                               'alias': item_data_name,\n",
    "                               'entities': [item_data_id],\n",
    "                               'probabilities': [1.0]\n",
    "                              }\n",
    "                    insere_alias_db(alias_kb, a_jsonl)\n",
    "\n",
    "                    # Se o titulo for diferente dos labels da wikidata\n",
    "                    # insere um alias equivalente            \n",
    "                    #if any([title.lower() != l.lower() for l in labels_item]):\n",
    "\n",
    "                    for alias_l in [l for l in item_data_labels if item_data_name.lower() != l.lower()]:\n",
    "                        a_jsonl = {\n",
    "                                   'alias': alias_l,\n",
    "                                   'entities': [item_data_id],\n",
    "                                   'probabilities': [1.0]\n",
    "                                  }\n",
    "                        insere_alias_db(alias_kb, a_jsonl)\n",
    "\n",
    "                    # Insere os alias\n",
    "                    for alias_l in item_data_aliases:\n",
    "                        a_jsonl = {\n",
    "                                   'alias': alias_l,\n",
    "                                   'entities': [item_data_id],\n",
    "                                   'probabilities': [1.0]\n",
    "                                  }            \n",
    "                        insere_alias_db(alias_kb, a_jsonl)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Erro processando entidade {t_ent}\")\n",
    "            print(f\"Erro: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-dispatch",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"titulo1\"\n",
    "labels_t = [\"Titulo\", \"Titulo1\"]\n",
    "\n",
    "any([x.lower() == t.lower() for x in labels_t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-clock",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(redacoes[167])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-commissioner",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing \n",
    "\n",
    "GLOBALLOCK = multiprocessing.Lock()   \n",
    "\n",
    "def worker_processa_redacao(l_redacoes):\n",
    "    \n",
    "    GLOBALLOCK.acquire()\n",
    "    \n",
    "    kb_mongo = get_mongo_db(\"base_conhecimento\")\n",
    "    raw_kb = get_mongo_collection(\"raw_kb\", kb_mongo)\n",
    "    entidades_kb = get_mongo_collection(\"entidades_kb\", kb_mongo)\n",
    "    alias_kb = get_mongo_collection(\"alias_kb\", kb_mongo)\n",
    "    \n",
    "    GLOBALLOCK.release()\n",
    "    \n",
    "    for redacao in l_redacoes:    \n",
    "        print()\n",
    "        print(50*\"=\")\n",
    "        print(f\"Processando redação ...\")\n",
    "        print(50*\"=\")\n",
    "        entidades_dbpedia = dbpedia.get_dbpedia_entries(redacao)\n",
    "\n",
    "        for t_ent, d_ent in entidades_dbpedia.items():\n",
    "            try:\n",
    "                print(f\"Processando entidade {t_ent}\")\n",
    "\n",
    "                id_inserido = raw_kb.insert_one(d_ent).inserted_id\n",
    "                print(f\"Inserido id {id_inserido}\")\n",
    "\n",
    "                tipo, item_id, title = get_wikipedia_page(d_ent['dbpedia_text'])\n",
    "                wikidata_item = get_wikidata_aliases(item_id)            \n",
    "                labels_item   = get_field_values(wikidata_item, 'labels')    \n",
    "                descriptions_item = \" \".join(get_field_values(wikidata_item, 'descriptions'))\n",
    "                aliases_item = get_field_values(wikidata_item, 'aliases')\n",
    "\n",
    "                item_data = {'id': item_id,\n",
    "                             'title': title,\n",
    "                             'labels': labels_item,\n",
    "                             'descriptions': descriptions_item,\n",
    "                             'aliases': aliases_item}\n",
    "\n",
    "                # Insere entidade \n",
    "                r_jsonl = {\n",
    "                           'id': item_id,\n",
    "                           'name': title,\n",
    "                           'description': descriptions_item,\n",
    "                           'label': \"\",  # este label é um indicador da base de conhecimento \n",
    "                          }\n",
    "                \n",
    "                ####################\n",
    "                GLOBALLOCK.acquire()\n",
    "                ####################    \n",
    "                \n",
    "                insere_entidade_db(entidades_kb, r_jsonl)\n",
    "\n",
    "                # Insere um alias equivalente a entidade\n",
    "                a_jsonl = {\n",
    "                           'alias': title,\n",
    "                           'entities': [item_id],\n",
    "                           'probabilities': [1.0]\n",
    "                          }\n",
    "                insere_alias_db(alias_kb, a_jsonl)\n",
    "\n",
    "                # Se o titulo for diferente dos labels da wikidata\n",
    "                # insere um alias equivalente\n",
    "                if not(title in labels_item):\n",
    "                    for alias_l in labels_item:\n",
    "                        a_jsonl = {\n",
    "                                   'alias': alias_l,\n",
    "                                   'entities': [item_id],\n",
    "                                   'probabilities': [1.0]\n",
    "                                  }            \n",
    "                        insere_alias_db(alias_kb, a_jsonl)\n",
    "\n",
    "\n",
    "                # Insere os alias\n",
    "                for alias_l in aliases_item:\n",
    "                    a_jsonl = {\n",
    "                               'alias': alias_l,\n",
    "                               'entities': [item_id],\n",
    "                               'probabilities': [1.0]\n",
    "                              }            \n",
    "                    insere_alias_db(alias_kb, a_jsonl)\n",
    "                    \n",
    "                ####################    \n",
    "                GLOBALLOCK.release()\n",
    "                ####################\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Erro processando entidade {t_ent}\")\n",
    "                print(f\"Erro: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-liabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_procs = 20\n",
    "n = len(redacoes)//num_procs\n",
    "chunks_redacoes = [redacoes[i * n:(i + 1) * n] for i in range((len(redacoes) + n - 1) // n )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks_redacoes)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-paste",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = multiprocessing.Pool(num_procs)\n",
    "p.map(worker_processa_redacao, chunks_redacoes)\n",
    "p.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
