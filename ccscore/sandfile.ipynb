{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testes básicos do funcionamento do CCScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'infernal'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ff0b7f3c45e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mccscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfernal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeature_extraction\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mccscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfernal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatastructures\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/desenv/master/ccscore/ccscore/infernal/feature_extraction.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumerals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatastructures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopenwordnetpt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/desenv/master/ccscore/ccscore/infernal/datastructures.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0menum\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEnum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0minfernal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlemmatization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0minfernal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopenwordnetpt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0minfernal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mppdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'infernal'"
     ]
    }
   ],
   "source": [
    "from infernal import feature_extraction as fe\n",
    "from infernal import datastructures as ds\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('pt_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalações de pacotes\n",
    "#!python -m spacy download pt_core_news_lg\n",
    "#!python -m spacy download pt_core_news_lg\n",
    "#!pip install zss rdflib\n",
    "#!conda install -y scipy\n",
    "#!conda install -y pandas\n",
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Defendida por agentes do mercado financeiro e uma das bandeiras da equipe econômica do governo Jair Bolsonaro, o projeto de autonomia do Banco Central (BC) deve avançar na Câmara só após a reforma tributária andar, no que depender do presidente da Casa, Rodrigo Maia (DEM-RJ). Para ele, o projeto sobre a instituição presidida por Roberto Campos Neto não é urgente no curto prazo.\\n\"Aceito votar autonomia do Banco, aceito, é claro, votar os depósitos voluntários, mas aí temos que organizar melhor a pauta até o fim do ano. É só o governo ter boa vontade na reforma tributária\", disse Maia, ao participar de evento organizado pelo Itaú. \"A reforma tributária tem importância muito maior que autonomia do Banco Central\", comentou.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto = '''Defendida por agentes do mercado financeiro e uma das bandeiras da equipe econômica do governo Jair Bolsonaro, o projeto de autonomia do Banco Central (BC) deve avançar na Câmara só após a reforma tributária andar, no que depender do presidente da Casa, Rodrigo Maia (DEM-RJ). Para ele, o projeto sobre a instituição presidida por Roberto Campos Neto não é urgente no curto prazo.\n",
    "\"Aceito votar autonomia do Banco, aceito, é claro, votar os depósitos voluntários, mas aí temos que organizar melhor a pauta até o fim do ano. É só o governo ter boa vontade na reforma tributária\", disse Maia, ao participar de evento organizado pelo Itaú. \"A reforma tributária tem importância muito maior que autonomia do Banco Central\", comentou.\n",
    "'''\n",
    "\n",
    "#O projeto de autonomia do BC foi aprovado na terça-feira, 3, pelo Senado e agora precisa do aval dos deputados para virar lei. O texto mantém o controle dos preços como objetivo central, mas inclui ainda duas novas metas acessórias, sem prejuízo à principal: suavizar as flutuações do nível de atividade econômica e fomentar o pleno emprego no País. O governo concordou com a redação da proposta, apesar de o BC ser historicamente contrário a ampliar o escopo da atuação.\n",
    "#Maia já reclamou outras vezes da falta de empenho e atuação do governo para se aprovar a medida. Na semana passada, acusou o presidente do Banco Central, Roberto Campos Neto, de ter vazado informações sobre conversa que os dois tiveram no dia da decisão do Comitê de Política Monetária (Copom), que manteve a taxa Selic em 2% ao ano.\n",
    "#Ao jornal O Estado de S. Paulo, Maia criticou a articulação do presidente do BC em alertar sobre os reflexos para a economia da dificuldade do Congresso em avançar com as votações da pauta de ajuste fiscal. Segundo o presidente da Câmara, Campos Neto tentou fazer uma articulação política, sem combinar, o que não seria papel dele, mas dos ministros da Economia, Paulo Guedes, e da articulação política, Luiz Eduardo Ramos.\n",
    "#Nesta sexta-feira, o presidente da Câmara lembrou que havia uma proposta de autonomia do BC semelhante na Câmara, mas que não foi votada, e disse que não comentou até agora sobre o tema porque não foi procurado pelo governo para falar sobre o assunto. \"Se eu conseguisse conversar com alguém do governo, eu poderia te responder, mas ninguém me procura. Não vou conversar com a imprensa antes de conversar com o governo\", disse.\n",
    "#Como o Broadcast (sistema de notícias em tempo real do Grupo Estado) mostrou na quinta-feira, deputados já se articulam na Câmara para modificar o projeto aprovado pelo Senado. O partido Novo, por exemplo, quer enxugar a proposta que recebeu aval dos senadores para reduzir os chamados acessórios que foram colocados para o Banco Central.\n",
    "#Sobre a reforma tributária, Maia deu sinais de que quer aprovar o projeto antes de deixar a presidência da Casa e acredita que com acordo pode fazer isso rapidamente.'''\n",
    "#'''\n",
    "\n",
    "texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processamento básico dos textos é feito com Spacy, \n",
    "mas de forma interno às classes do ccscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdoc = nlp(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paragraphs(document):\n",
    "    '''\n",
    "    Generator to define start positions\n",
    "    of paragraphs\n",
    "    '''\n",
    "    start = 0\n",
    "    for token in document:\n",
    "        if token.is_space and token.text.count(\"\\n\") > 1:\n",
    "            yield document[start:token.i]\n",
    "            start = token.i\n",
    "    yield document[start:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import tee\n",
    "\n",
    "def pairwise(iterable):\n",
    "    '''\n",
    "    Receive an iterable and return\n",
    "    an list with tuples of this elements\n",
    "    is this way    \n",
    "    s -> (s0,s1), (s1,s2), (s2, s3), ...\n",
    "    '''\n",
    "    a, b = tee(iterable)    \n",
    "    next(b, None)  # next(b) it's Ok \n",
    "    return zip(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './ccscore/data/tep2.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1f643b055872>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtext_document\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextDocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtep2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGrupoSinonimo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/desenv/master/ccscore/ccscore/text_document.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msplit_utils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msingle_sentence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSingleSentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfapp\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtep2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGrupoSinonimo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/desenv/master/ccscore/ccscore/single_sentence.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdependency\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDependency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpermutations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhelper_tools\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhtools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtep2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGrupoSinonimo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0minfernal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopenwordnetpt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/desenv/master/ccscore/ccscore/helper_tools.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mdic_tep2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEP2_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mdic_tep2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './ccscore/data/tep2.pickle'"
     ]
    }
   ],
   "source": [
    "from ccscore.text_document import TextDocument\n",
    "\n",
    "td = TextDocument(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Listando:    \n",
    "<ol>\n",
    "    <li>Foco Explícito (FE)</li>\n",
    "    <li>Lista intermediária do FE</li>    \n",
    "    <li>Lista das Entidades Nomeadas</li>\n",
    "    <li>Menções da DBPedia</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-22b56d130e73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#td.sentences[0].tokens_fe_pos_tags['agentes'].pos_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens_fe_pos_tags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "#td.sentences[0].tokens_fe_pos_tags['agentes'].pos_\n",
    "td.sentences[0].tokens_fe_pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, sent in enumerate(td.sentences):\n",
    "    print()\n",
    "    print(\"ID: \", str(i))    \n",
    "    print(\"Texto Original: \", sent.text)    \n",
    "    print()    \n",
    "    print(\"Lista de Foco Explícito: \", [s for s in sent.list_fe])    \n",
    "    print()\n",
    "    print(\"Lista de Intermediária de FE: \", sent.list_fe_li)#[s for s in sent.list_fe_li])    \n",
    "    print()    \n",
    "    print(\"Lista de Entidades Nomeadas: \", sent.named_entities)    \n",
    "    print()\n",
    "    print(\"Lista de Menções DBPedia: \", str(sent.dbpedia_mentions))    \n",
    "    \n",
    "    print(\"-\"*100)\n",
    "    \n",
    "#s_disp = td.nlp_processor(td.sentences[4].text)\n",
    "#spacy.displacy.render(s_disp, style=\"dep\", options = {\"compact\": True})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste do extrator de features do pacote infernal\n",
    "extrator = fe.FeatureExtractor(both=True)\n",
    "extrator.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from infernal.datastructures import Entailment \n",
    "s1,s2 = pares[0]\n",
    "\n",
    "\n",
    "par = ds.Pair(s1,s2, 0, Entailment.none)\n",
    "\n",
    "par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_ex = None\n",
    "for i,s in enumerate(textdoc.sents):\n",
    "    s_ex = s\n",
    "    if i == 1:\n",
    "        break\n",
    "   # print(dir(s))\n",
    "    print(type(s))\n",
    "\n",
    "#dir(s_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper_tools as htools\n",
    "from tep2 import GrupoSinonimo\n",
    "\n",
    "htools.dic_tep2.get_sinonimos(\"governo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, sent in enumerate(td.sentences):\n",
    "    print()\n",
    "    print(\"ID: \", str(i))   \n",
    "    print(sent.tokens_dbpedia_metions)\n",
    "    print(\"Texto Original: \", sent.text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import helper_tools as htools\n",
    "\n",
    "#htools.get_dbpedia_mentions(texto)\n",
    "dbpedia_mentions = []\n",
    "dbpedia_mentions_entries = htools.get_dbpedia_entries(texto)\n",
    "for key, entry in dbpedia_mentions_entries.items():\n",
    "    if entry and key == 'raw_text':        \n",
    "        dbpedia_mentions.append(entry['raw_text'].replace(\"'\",''))\n",
    "\n",
    "#print(dbpedia_mentions)\n",
    "#print(dbpedia_mentions_entries)\n",
    "\n",
    "print(\"\\n\\n\".join([\"{}: {}\".format(k,v) for k,v in dbpedia_mentions_entries.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "page = requests.get(\"http://pt.dbpedia.org/resource/Pleno_emprego\")\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "a_elements = soup.find_all('a', text=re.compile(\".*Categoria.*\"))\n",
    "\n",
    "def get_categorias_dbpedia(resource):\n",
    "    page = requests.get(\"http://pt.dbpedia.org/resource/{}\".format(resource))\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    a_elements = soup.find_all('a', text=re.compile(\".*Categoria.*\"))\n",
    "    \n",
    "    categorias = set([])\n",
    "    for elem in a_elements:\n",
    "        texto = \"\".join(elem.text.split())\n",
    "        texto = texto.split(\"dbr:Categoria:\")[1]\n",
    "        texto = texto.replace(\"_\", \" \")\n",
    "        categorias.add(texto)\n",
    "\n",
    "    return categorias\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categorias = set([])\n",
    "for elem in a_elements:\n",
    "    texto = \"\".join(elem.text.split())\n",
    "    texto = texto.split(\"dbr:Categoria:\")[1]\n",
    "    texto = texto.replace(\"_\", \" \")\n",
    "    categorias.add(texto)\n",
    "    \n",
    "categorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_categorias_dbpedia(\"Reforma_tributária\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print(list(dbpedia_mentions_entries.values())[5])\n",
    "print(htools.get_dbpedia_entries(texto, formated=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperar os tokens que constituem os spans relacionados\n",
    "# as menções obtidas da dbpedia spotlight\n",
    "\n",
    "ent1 = list(dbpedia_mentions_entries.values())[0]\n",
    "\n",
    "tk0 = None\n",
    "for t in td.sentences[0].annotated:\n",
    "    if  t.idx == int(ent1['pos']):\n",
    "        tk0 = t\n",
    "\n",
    "\n",
    "sp1 = td.sentences[0].annotated[tk0.i:tk0.i+len(ent1['raw_text'].split())]\n",
    "for t in sp1:\n",
    "    print(type(t))\n",
    "    print(t.pos_)\n",
    "    print(t.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from single_sentence import SingleSentence\n",
    "\n",
    "for i,s in enumerate(td.sentences):\n",
    "    if i == 1:\n",
    "        break\n",
    "\n",
    "id_sent = 1        \n",
    "    \n",
    "    \n",
    "#for t in td.sentences[id_sent].list_fe:    \n",
    "#    print(f\"{t} -- {t.pos_} -- {t.dep_}\")\n",
    "    \n",
    "#print(td.sentences[0].list_fe)    \n",
    "#s_ano = td.nlp_processor(td.sentences[0].text)\n",
    "#spacy.displacy.render(s_ano, style=\"dep\", options = {\"compact\": True})\n",
    "\n",
    "#print(td.sentences[1].list_fe)    \n",
    "#s_ano = td.nlp_processor(td.sentences[1].text)\n",
    "#spacy.displacy.render(s_ano, style=\"dep\", options = {\"compact\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#syns = own.find_synonyms(\"governo\")\n",
    "import helper_tools as htools\n",
    "\n",
    "\n",
    "for sent in td.sentences:\n",
    "    print(\"=\"*100)\n",
    "    print(f\"Sentenca: {sent.text}\")\n",
    "    print(\"=\"*100, '\\n')\n",
    "    \n",
    "    for fe_elem in sent.list_fe:\n",
    "        print(fe_elem, \":\")\n",
    "        try:\n",
    "            print(f\"WordNet: {own.find_synonyms(htools.cogroo_lemmatize(fe_elem.lower()))}\")\n",
    "            print(f\"TeP2: {tep2.get_sinominos(htools.cogroo_lemmatize(fe_elem.lower()))}\")\n",
    "        except:\n",
    "            print(None)\n",
    "        print(\"-------------------------\")\n",
    "#own.are_synonyms(\"governo\", \"instituição\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(td.sentences[0].dbpedia_mentions[0])\n",
    "for i, sent in enumerate(td.sentences):\n",
    "    for text_token in sent.list_fe:    \n",
    "        pos_token = sent.tokens_fe_pos_tags[text_token].pos_ if text_token in sent.tokens_fe_pos_tags.keys() else None\n",
    "        print(text_token, pos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = td.sentences[0]\n",
    "\n",
    "import itertools as itt\n",
    "\n",
    "it_fe_pairs = itt.permutations(sent.list_fe, 2)\n",
    "\n",
    "\n",
    "#print([elem_fe_pair for elem_fe_pair in it_fe_pairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as itt\n",
    "\n",
    "it_fe_pairs = itt.permutations(sent.list_fe, 2)\n",
    "head_elem = None\n",
    "is_part_of_other = False\n",
    "elems_to_exclude = set([])\n",
    "for a,b in it_fe_pairs:    \n",
    "    if len(a.split()) == 1:    \n",
    "        if a in b:\n",
    "            elems_to_exclude.add(a)\n",
    "            is_part_of_other = True            \n",
    "            \n",
    "print(sent.list_fe,' - ', len(sent.list_fe))    \n",
    "list_fe = list(set(sent.list_fe).difference(set(elems_to_exclude)))\n",
    "print(list_fe,' - ', len(list_fe))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = td.sentences[3]\n",
    "\n",
    "#print(dir(ss.anotated))\n",
    "#ss.anotated.start\n",
    "\n",
    "sentence_tokens = [token for token in ss.annotated]\n",
    "type(sentence_tokens[0])\n",
    "sentence_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(text):\n",
    "    doc = nlp(text)\n",
    "    return (list(doc.sents)) # convert to string if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\"É só o governo ter boa vontade na reforma tributária\", disse Maia, ao participar de evento organizado pelo Itaú. \n",
    "\"A reforma tributária tem importância muito maior que autonomia do Banco Central\", comentou. O projeto de autonomia do BC foi aprovado na terça-feira, 3, pelo Senado e agora precisa do aval dos deputados para virar lei.'''\n",
    "\n",
    "#get_sentences(text)\n",
    "doc = nlp(text)\n",
    "sents = [sent.text for sent in doc.sents]\n",
    "print(\"After:\", sents)\n",
    "print(\"tamanho:\", len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import split_utils as su\n",
    "su.split_by_sentence(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from infernal import openwordnetpt as own\n",
    "own.load_wordnet(\"./data/own-pt.pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(own.find_synonyms(\"reitor\"))\n",
    "except:\n",
    "    print(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations \n",
    "from collections import defaultdict \n",
    "\n",
    "class GrupoSinonimo():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data = {}\n",
    "        self.sinonimos = defaultdict(list)\n",
    "        self.classes = defaultdict(str)\n",
    "        self.idx_antonimos = defaultdict(int)\n",
    "        \n",
    "    def add(self, idx, dados_grupo):\n",
    "        if dados_grupo:            \n",
    "            self.data[idx] = dados_grupo\n",
    "            self.set_sinonimos(idx)\n",
    "            self.set_classes(idx)\n",
    "        else:\n",
    "            print(\"Sem dados para inserir\")\n",
    "    \n",
    "    def get(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def set_sinonimos(self, idx):               \n",
    "        all_sinoms = self.data[idx][1]\n",
    "        \n",
    "        pares_palavras = combinations(all_sinoms, 2)\n",
    "        for sinonimo_left, sinonimo_right in pares_palavras:\n",
    "            self.sinonimos[sinonimo_left].append(sinonimo_right)\n",
    "            self.sinonimos[sinonimo_right].append(sinonimo_left)\n",
    "                \n",
    "    def get_sinonimos(self, palavra):\n",
    "        return self.sinonimos[palavra]\n",
    "    \n",
    "    def set_classes(self, idx):\n",
    "        all_sinoms = self.data[idx][1]\n",
    "        classe = self.data[idx][0]\n",
    "                \n",
    "        for palavra in all_sinoms:\n",
    "            #print(\"classe: \" + classe + \" \" + \"palavra: \" + palavra)\n",
    "            self.classes[palavra] = classe        \n",
    "    \n",
    "    def get_classe(self, palavra):\n",
    "        return self.classes[palavra]\n",
    "    \n",
    "    def set_antonimos(self, idx):\n",
    "        idx_antonimo = self.data[idx][2]\n",
    "        all_sinoms = self.data[idx][1]\n",
    "        \n",
    "        for palavra in all_sinoms:\n",
    "            self.idx_antonimos[palavra] = idx_antonimo\n",
    "            \n",
    "    def get_antonimos(self, palavra):\n",
    "        if palavra in self.idx_antonimos.keys():\n",
    "            data = self.get(self.idx_antonimos[palavra])        \n",
    "            return data[1]\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def get_by_classe(self, classe):\n",
    "        return [k for k,v in self.classes.items() if v == classe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "tep2 = None\n",
    "with open('./data/tep2.pickle', 'rb') as f:\n",
    "    tep2 = pickle.load(f)\n",
    "    \n",
    "tep2 = tep2['objeto']\n",
    "\n",
    "tep2.get_sinominos('governo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cogroo_interface import Cogroo\n",
    "cogroo = Cogroo.Instance()\n",
    "doc = cogroo.analyze('bandeiras')\n",
    "print(f\"token pos: {doc.sentences[0].tokens}\")\n",
    "print(f\"token lemma: {cogroo.lemmatize('bandeiras')}\")\n",
    "\n",
    "print(f\"token lemma: {cogroo.lemmatize('abacelara-lhe')}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FE_POS_TAGS = {\"NOUN\", \"PRON\", \"PROPN\"}\n",
    "POS_PLURAL = {}\n",
    "tk_exemplo = None\n",
    "for sent in td.sentences:\n",
    "    print(f\"Sentenca: {sent.text}\")\n",
    "    for token in sent.annotated:\n",
    "        #if token.text == \"agentes\":\n",
    "                #print(token.text, \" \", token.tag_)\n",
    "                                \n",
    "                if tk_exemplo is None:\n",
    "                    tk_exemplo = token\n",
    "                if token.pos_ in FE_POS_TAGS and \\\n",
    "                   \"Number=Plur\" in token.tag_ :# and token.tag_ in POS_PLURAL:\n",
    "                    print(f\"Texto: {token.text}\")\n",
    "                    print(f\"Lemma: {token.lemma_}\")\n",
    "                    print(f\"Morph: {token.text}\")\n",
    "                    print(f\"POS: {token.pos_}\")\n",
    "                    print(f\"Lemma CoGroo: {cogroo.lemmatize(token.text)}\")\n",
    "                    print(\"--\"*30)\n",
    "\n",
    "#print(f\"Texto: {tk_exemplo.text}\")\n",
    "#print(f\"Lemma: {tk_exemplo.lemma_}\")\n",
    "#print(f\"Morph: {tk_exemplo.text}\")\n",
    "\n",
    "#dir(tk_exemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join([l for l in nlp.get_pipe(\"tagger\").labels if \"Number=Plur\" in l]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from infernal import lemmatization\n",
    "\n",
    "lemmatization.get_lemma(\"depósitos\", \"NOUN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from itertools import tee\n",
    "#random.randint(10, 100)\n",
    "l = [x for x in range(5)]\n",
    "a,b = tee(l)\n",
    "next(b)\n",
    "\n",
    "list(zip(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentencas = []\n",
    "\n",
    "add_asspas = False\n",
    "for s in textdoc.sents:\n",
    "    if s.text == '\"':\n",
    "        add_asspas = True\n",
    "    else:\n",
    "        if add_asspas:\n",
    "            sentencas.append('\"'+s.text)\n",
    "            add_asspas = False\n",
    "        else:\n",
    "            sentencas.append(s.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
